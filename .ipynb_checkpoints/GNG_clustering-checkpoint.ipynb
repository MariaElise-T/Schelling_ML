{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "600b48a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Main information\n",
      "\n",
      "[ALGORITHM] GrowingNeuralGas\n",
      "\r\n",
      "[OPTION] verbose = True\n",
      "[OPTION] epoch_end_signal = None\n",
      "[OPTION] show_epoch = 1\n",
      "[OPTION] shuffle_data = True\n",
      "[OPTION] train_end_signal = None\n",
      "[OPTION] after_split_error_decay_rate = 0.5\n",
      "[OPTION] error_decay_rate = 0.995\n",
      "[OPTION] max_edge_age = 50\n",
      "[OPTION] max_nodes = 100\n",
      "[OPTION] min_distance_for_update = 0.05\n",
      "[OPTION] n_inputs = 10\n",
      "[OPTION] n_iter_before_neuron_added = 100\n",
      "[OPTION] n_start_nodes = 5\n",
      "[OPTION] neighbour_step = 0.01\n",
      "[OPTION] step = 0.1\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "from neupy import algorithms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "from collections.abc import Iterable, MutableMapping as DictMixin\n",
    "import os\n",
    "\n",
    "\n",
    "gng = algorithms.GrowingNeuralGas(\n",
    "    n_inputs=10,\n",
    "    n_start_nodes=5,\n",
    "    shuffle_data=True,\n",
    "    verbose=True,\n",
    "    step=0.1,\n",
    "    neighbour_step=0.01,\n",
    "    max_edge_age=50,\n",
    "    max_nodes=100,\n",
    "    n_iter_before_neuron_added=100,\n",
    "    after_split_error_decay_rate=0.5,\n",
    "    error_decay_rate=0.995,\n",
    "    min_distance_for_update=0.05\n",
    " )\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e2124fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/maria/Desktop/Schelling_ML\")\n",
    "train_inputs = pd.read_csv('train_inputs_hap_0.csv')  \n",
    "train_outputs = pd.read_csv('train_outputs_hap_0.csv')  \n",
    "train_inputs2 = pd.read_csv('train_inputs_homo_0.csv')  \n",
    "for i in range(1,100):\n",
    "    filepath_in = 'train_inputs_hap_'+str(i)+'.csv'\n",
    "    filepath_out = 'train_outputs_hap_'+str(i)+'.csv'\n",
    "    filepath_in2 = 'train_inputs_homo_'+str(i)+'.csv'\n",
    "    file_in = pd.read_csv(filepath_in)\n",
    "    file_out = pd.read_csv(filepath_out)\n",
    "    file_in2 = pd.read_csv(filepath_in2)\n",
    "    train_inputs = pd.concat([train_inputs, file_in])\n",
    "    train_outputs = pd.concat([train_outputs, file_out])\n",
    "    train_inputs2 = pd.concat([train_inputs2, file_in2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "810411d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.652377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.674791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.595799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.143759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.414441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.119902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.438393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.698130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.269754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0   0.652377\n",
       "1   0.062143\n",
       "2   0.674791\n",
       "3   0.595799\n",
       "4   0.143759\n",
       "..       ...\n",
       "95  0.414441\n",
       "96  0.119902\n",
       "97  0.438393\n",
       "98  0.698130\n",
       "99  0.269754\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa556e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training\n",
      "\n",
      "[TRAINING DATA] shapes: (10000, 10)\n",
      "[TRAINING] Total epochs: 100\n",
      "\n",
      "---------------------------------------------------------\n",
      "|    Epoch    |  Train err  |  Valid err  |    Time     |\n",
      "---------------------------------------------------------\n",
      "|           1 |       3.593 |           - |      981 ms |\n",
      "|           2 |      2.7789 |           - |       1 sec |\n",
      "|           3 |      2.7659 |           - |       1 sec |\n",
      "|           4 |      2.7681 |           - |       1 sec |\n",
      "|           5 |      2.7677 |           - |       1 sec |\n",
      "|           6 |      2.7625 |           - |       1 sec |\n",
      "|           7 |      2.7561 |           - |       1 sec |\n",
      "|           8 |        2.76 |           - |       1 sec |\n",
      "|           9 |      2.7582 |           - |       1 sec |\n",
      "|          10 |       2.751 |           - |       1 sec |\n",
      "|          11 |      2.7558 |           - |       1 sec |\n",
      "|          12 |      2.7572 |           - |       1 sec |\n",
      "|          13 |      2.7573 |           - |       1 sec |\n",
      "|          14 |      2.7511 |           - |       1 sec |\n",
      "|          15 |      2.7493 |           - |       1 sec |\n",
      "|          16 |      2.7484 |           - |       1 sec |\n",
      "|          17 |      2.7575 |           - |       1 sec |\n",
      "|          18 |      2.7591 |           - |       1 sec |\n",
      "|          19 |       2.755 |           - |       1 sec |\n",
      "|          20 |      2.7529 |           - |       1 sec |\n",
      "|          21 |      2.7572 |           - |       1 sec |\n",
      "|          22 |      2.7526 |           - |       1 sec |\n",
      "|          23 |      2.7518 |           - |       1 sec |\n",
      "|          24 |      2.7501 |           - |       1 sec |\n",
      "|          25 |      2.7506 |           - |       1 sec |\n",
      "|          26 |      2.7527 |           - |       1 sec |\n",
      "|          27 |      2.7579 |           - |       1 sec |\n",
      "|          28 |      2.7465 |           - |       1 sec |\n",
      "|          29 |      2.7581 |           - |       1 sec |\n",
      "|          30 |      2.7487 |           - |       1 sec |\n",
      "|          31 |      2.7484 |           - |       1 sec |\n",
      "|          32 |      2.7518 |           - |       1 sec |\n",
      "|          33 |      2.7437 |           - |       1 sec |\n",
      "|          34 |      2.7505 |           - |       1 sec |\n",
      "|          35 |      2.7436 |           - |       1 sec |\n",
      "|          36 |      2.7524 |           - |       1 sec |\n",
      "|          37 |      2.7459 |           - |       1 sec |\n",
      "|          38 |       2.748 |           - |       1 sec |\n",
      "|          39 |      2.7505 |           - |       1 sec |\n",
      "|          40 |      2.7551 |           - |       1 sec |\n",
      "|          41 |      2.7512 |           - |       1 sec |\n",
      "|          42 |      2.7496 |           - |       1 sec |\n",
      "|          43 |      2.7541 |           - |       1 sec |\n",
      "|          44 |      2.7595 |           - |       1 sec |\n",
      "|          45 |      2.7548 |           - |       1 sec |\n",
      "|          46 |      2.7571 |           - |       1 sec |\n",
      "|          47 |       2.753 |           - |       1 sec |\n",
      "|          48 |      2.7576 |           - |       1 sec |\n",
      "|          49 |       2.758 |           - |       1 sec |\n",
      "|          50 |      2.7577 |           - |       1 sec |\n",
      "|          51 |      2.7547 |           - |       1 sec |\n",
      "|          52 |      2.7521 |           - |       1 sec |\n",
      "|          53 |      2.7536 |           - |       1 sec |\n",
      "|          54 |      2.7478 |           - |       1 sec |\n",
      "|          55 |      2.7476 |           - |       1 sec |\n",
      "|          56 |      2.7469 |           - |       1 sec |\n",
      "|          57 |      2.7506 |           - |       1 sec |\n",
      "|          58 |      2.7589 |           - |       1 sec |\n",
      "|          59 |      2.7506 |           - |       1 sec |\n",
      "|          60 |      2.7543 |           - |       1 sec |\n",
      "|          61 |      2.7489 |           - |       1 sec |\n",
      "|          62 |      2.7533 |           - |       1 sec |\n",
      "|          63 |      2.7548 |           - |       2 sec |\n",
      "|          64 |      2.7568 |           - |       1 sec |\n",
      "|          65 |      2.7575 |           - |       1 sec |\n",
      "|          66 |      2.7601 |           - |       1 sec |\n",
      "|          67 |      2.7573 |           - |       1 sec |\n",
      "|          68 |      2.7504 |           - |       1 sec |\n",
      "|          69 |      2.7514 |           - |       1 sec |\n",
      "|          70 |      2.7486 |           - |       1 sec |\n",
      "|          71 |      2.7441 |           - |       1 sec |\n",
      "|          72 |      2.7496 |           - |       1 sec |\n",
      "|          73 |       2.754 |           - |       1 sec |\n",
      "|          74 |      2.7553 |           - |       1 sec |\n",
      "|          75 |      2.7539 |           - |       1 sec |\n",
      "|          76 |       2.753 |           - |       1 sec |\n",
      "|          77 |      2.7512 |           - |       1 sec |\n",
      "|          78 |      2.7562 |           - |       1 sec |\n",
      "|          79 |      2.7504 |           - |       1 sec |\n",
      "|          80 |      2.7489 |           - |       1 sec |\n",
      "|          81 |      2.7482 |           - |       1 sec |\n",
      "|          82 |      2.7534 |           - |       1 sec |\n",
      "|          83 |       2.755 |           - |       1 sec |\n",
      "|          84 |      2.7493 |           - |       1 sec |\n",
      "|          85 |      2.7536 |           - |       1 sec |\n",
      "|          86 |      2.7579 |           - |       1 sec |\n",
      "|          87 |      2.7457 |           - |       1 sec |\n",
      "|          88 |      2.7497 |           - |       1 sec |\n",
      "|          89 |      2.7523 |           - |       1 sec |\n",
      "|          90 |      2.7589 |           - |       1 sec |\n",
      "|          91 |       2.754 |           - |       1 sec |\n",
      "|          92 |      2.7554 |           - |       1 sec |\n",
      "|          93 |      2.7552 |           - |       1 sec |\n",
      "|          94 |      2.7564 |           - |       1 sec |\n",
      "|          95 |      2.7566 |           - |       1 sec |\n",
      "|          96 |      2.7544 |           - |       1 sec |\n",
      "|          97 |      2.7594 |           - |       1 sec |\n",
      "|          98 |      2.7512 |           - |       1 sec |\n",
      "|          99 |       2.754 |           - |       1 sec |\n",
      "|         100 |      2.7593 |           - |       1 sec |\n",
      "---------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gng.train(train_inputs, epochs=epoch) # only including features, features are named as '0', '1', '2' ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bea8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gng.graph\n",
    "node_list = g.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5db2a6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maria/anaconda3/lib/python3.10/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_static_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m df_scaled_sample_node_matching \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scaled_sample_node_matching)\n\u001b[1;32m     22\u001b[0m df_scaled_sample_node_matching \u001b[38;5;241m=\u001b[39m df_scaled_sample_node_matching\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{number_of_features: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m---> 23\u001b[0m df_node_sample_match \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\u001b[43mdf_static_cols\u001b[49m, df_scaled_sample_node_matching, left_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_static_cols' is not defined"
     ]
    }
   ],
   "source": [
    "number_of_features = 10\n",
    "node_koor_node = []\n",
    "for node in gng.graph.nodes:\n",
    "    row = node.weight[0].tolist()\n",
    "    row.append(node_list.index(node))\n",
    "    node_koor_node.append(row)\n",
    "df_train = pd.DataFrame(node_koor_node)\n",
    "df_test = pd.DataFrame(train_inputs)\n",
    "X_train = df_train.iloc[:, 0:number_of_features] \n",
    "y_train = df_train.iloc[:, number_of_features]\n",
    "X_test = df_test.iloc[:, 0:number_of_features]\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_result = knn.predict(X_test)\n",
    "scaled_sample_node_matching = []\n",
    "for i, data in enumerate(train_inputs):\n",
    "    row = list(data)\n",
    "    node_idx = knn_result[i]\n",
    "    row.append(node_idx)\n",
    "    scaled_sample_node_matching.append(row)\n",
    "df_scaled_sample_node_matching = pd.DataFrame(scaled_sample_node_matching)\n",
    "df_scaled_sample_node_matching = df_scaled_sample_node_matching.rename(columns={number_of_features: 'node'})\n",
    "df_node_sample_match = pd.merge(df_static_cols, df_scaled_sample_node_matching, left_index=True, right_index=True) # df_static_cols includes static columns and datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf640f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_inputs):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fcc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_list = df_node_sample_match['DEVICE'].unique()\n",
    "n = len(gng.graph.nodes)\n",
    "mat_transition = np.zeros((n, n))\n",
    "previous_node = -1\n",
    "for device in device_list:\n",
    "    df_device = df_node_sample_match.loc[df_node_sample_match['DEVICE'] == device]\n",
    "    df_device = df_device.sort_values('DATETIME')\n",
    "    for i, row in df_device.iterrows():\n",
    "        current_node = row['node']\n",
    "        if (previous_node != -1):\n",
    "            mat_transition[previous_node, current_node] = mat_transition[previous_node, current_node] + 1\n",
    "        else:\n",
    "            print('It is the starting node, no transaction!')\n",
    "        previous_node = current_node\n",
    "    previous_node = -1\n",
    "df_adjacency = pd.DataFrame(mat_transition)\n",
    "df_adjacency['sum_of_rows'] = df_adjacency.sum(axis=1)\n",
    "list_degree = df_adjacency['sum_of_rows'].tolist()\n",
    "degree_matrix = np.diag(list_degree)\n",
    "# calculating transition probability\n",
    "df_affinity = df_adjacency.loc[:, :].div(df_adjacency[\"sum_of_rows\"], axis=0)\n",
    "df_affinity = df_affinity.drop([\"sum_of_rows\"], axis=1)\n",
    "df_affinity = df_affinity.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da24a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.identity(df_affinity.shape[0])\n",
    "sqrt = np.sqrt(degree_matrix)\n",
    "D_inv_sqrt = np.linalg.inv(sqrt)\n",
    "normalised_laplace = I — np.dot(D_inv_sqrt, df_affinity).dot(D_inv_sqrt)\n",
    "df_normalised_laplace = pd.DataFrame(normalised_laplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ae484",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_nodes = df_affinity.shape[0]\n",
    "cluster_number = 10\n",
    "u, s, vN = np.linalg.svd(normalised_laplace, full_matrices=False)\n",
    "vN = np.transpose(vN)\n",
    "eigen_vecs = vN[:, number_of_nodes — cluster_number:number_of_nodes] # 10 eigenvectors with smallest 10 eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9435bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "kmeans = KMeans(n_clusters=cluster_number, max_iter=1000, random_state=0)\n",
    "kmeans.fit(eigen_vecs)\n",
    "labels = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "# node-cluster matching\n",
    "clusters = dict()\n",
    "for i, n in enumerate(node_list):\n",
    "    if labels[i] not in clusters:\n",
    "        clusters[labels[i]] = list()\n",
    "    clusters[labels[i]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We somehow determine the anomaly clusters\n",
    "anomaly_clusters_list = [3, 4, 5, 7, 8, 9]\n",
    "normal_cluster_list = [0, 1, 2, 6]\n",
    "node_cluster_matching = []\n",
    "for i in range(len(node_list)):\n",
    "    for j in range(cluster_number):\n",
    "        if i in clusters[j]:\n",
    "    node_cluster_matching.append([i, j])\n",
    "df_node_cluster_matching = pd.DataFrame(node_cluster_matching)\n",
    "df_node_cluster_matching.columns = [‘node’, ‘cluster’]\n",
    "df_sample_node_cluster = pd.merge(df_node_sample_match, df_node_cluster_matching, on=[‘node’], how=’inner’)\n",
    "list_ = np.arange(0, number_of_features, 1).tolist()\n",
    "list_ = [str(i) for i in list_]\n",
    "list_.append(‘cluster’)\n",
    "df_anomaly_clusters_data = df_sample_node_cluster.loc[\n",
    "df_sample_node_cluster[‘cluster’].isin(anomaly_clusters_list)]\n",
    "df_anomaly_clusters_data = df_anomaly_clusters_data.loc[:, df_anomaly_clusters_data.columns.isin(list_)]\n",
    "df_anomaly_clusters_data = df_anomaly_clusters_data.reset_index(drop=True)\n",
    "df_normal_clusters_data = df_sample_node_cluster.loc[\n",
    "df_sample_node_cluster[‘cluster’].isin(normal_cluster_list)]\n",
    "df_normal_clusters_data = df_normal_clusters_data.loc[:, df_normal_clusters_data.columns.isin(list_)]\n",
    "df_normal_clusters_data = df_normal_clusters_data.reset_index(drop=True)\n",
    "dict_of_cluster_feature_pvaluelist = dict()\n",
    "for c in anomaly_clusters_list:\n",
    "    filtered_anomaly_data = df_anomaly_clusters_data.loc[df_anomaly_clusters_data[‘cluster’] == c]\n",
    "    pvalue_feature_list = []\n",
    "    for i in range(number_of_features):\n",
    "    anomaly_clusters_data_feature = filtered_anomaly_data[str(i)].to_numpy().tolist()\n",
    "    normal_clusters_data_feature = df_normal_clusters_data[str(i)].to_numpy().tolist()\n",
    "    result = stats.ks_2samp(anomaly_clusters_data_feature, normal_clusters_data_feature)\n",
    "    print(‘Result of cluster ‘ + str(c) + ‘ feature ‘ + str(i))\n",
    "    print(result)\n",
    "    pvalue = result.pvalue\n",
    "    if pvalue <= 0.05:\n",
    "    pvalue_feature_list.append([i, pvalue])\n",
    "    pvalue_feature_list.sort(key=lambda x: x[1])\n",
    "    pvalue_feature_list = pvalue_feature_list[0: 3] # lets say we pick top 3 probable features for root cause \n",
    "    dict_of_cluster_feature_pvaluelist[‘c’ + str(c)] = pvalue_feature_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
